{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Python Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part does not depend on the TensorFlow version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(\"Tensorflow version: \", tf.version.VERSION)\n",
    "\n",
    "# remember the tf version for further use\n",
    "TF_VER = tf.version.VERSION.split(\".\")[0]\n",
    "\n",
    "# Other supporting libraries\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be working with the <a href=\"https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/\">Fashion-MNIST</a> dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    # Add a new axis\n",
    "    train_images = train_images[:, :, :, np.newaxis]\n",
    "    test_images = test_images[:, :, :, np.newaxis]\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    train_labels = to_categorical(train_labels, num_classes)\n",
    "    test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "    # Data normalization\n",
    "    train_images = train_images.astype('float32')\n",
    "    test_images = test_images.astype('float32')\n",
    "    train_images /= 255\n",
    "    test_images /= 255\n",
    "    \n",
    "    #Note: TensorRT only supports 'channels first' input data type\n",
    "    train_images = np.rollaxis(train_images, 3, 1) \n",
    "    test_images = np.rollaxis(test_images, 3, 1) \n",
    "    \n",
    "    return train_images,train_labels, test_images, test_labels\n",
    "\n",
    "# Load data\n",
    "train_images,train_labels, test_images, test_labels = load_data()\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT supported operations: [https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 model\n",
    "class LeNet(keras.Sequential):\n",
    "    def __init__(self, input_shape, nb_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.add(keras.layers.InputLayer(input_shape=input_shape, name=\"InputLayer\"))\n",
    "        self.add(keras.layers.Conv2D(filters=32, kernel_size=(5,5), padding='same', activation=tf.nn.tanh, data_format='channels_first'))\n",
    "        self.add(keras.layers.MaxPool2D(strides=2))\n",
    "        self.add(keras.layers.Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation=tf.nn.tanh))\n",
    "        self.add(keras.layers.MaxPool2D(strides=2))\n",
    "        \n",
    "        self.add(keras.layers.Flatten(name=\"flatten\"))\n",
    "        self.add(keras.layers.Dense(120, activation=tf.nn.tanh))\n",
    "        self.add(keras.layers.Dense(84, activation=tf.nn.tanh))\n",
    "        self.add(keras.layers.Dense(nb_classes))\n",
    "        self.add(keras.layers.Activation(activation=tf.nn.softmax, name=\"OutputLayer\"))\n",
    "\n",
    "        self.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.categorical_crossentropy,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(train_images[0].shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_images, y=train_labels, \n",
    "            epochs=5, \n",
    "            validation_data=(test_images, test_labels), \n",
    "            verbose=1)\n",
    "    \n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test loss: {}\\nTest accuracy: {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_mkdir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        \n",
    "MODEL_DIR = \"./models\"  \n",
    "\n",
    "# Make directory to save model in if it doesn't exist already\n",
    "maybe_mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the TensorFlow version, save model as SavedModel for TF 2.x and as frozen graph for TF 1.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_tf1(model, model_path):\n",
    "    output_names = model.output.op.name\n",
    "    sess = tf.keras.backend.get_session()\n",
    "\n",
    "    graphdef = sess.graph.as_graph_def()\n",
    "    \n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(sess, graphdef, [output_names])\n",
    "    frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "    \n",
    "    with open(model_path, \"wb\") as ofile:\n",
    "        ofile.write(frozen_graph.SerializeToString())\n",
    "\n",
    "        \n",
    "def saved_frozen_graph_tf2(model, logdir, name, verbose=True):\n",
    "    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "    \n",
    "    # Convert Keras model to ConcreteFunction\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "    full_model = full_model.get_concrete_function(\n",
    "        tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=\"InputLayer\"))\n",
    "\n",
    "     # Get frozen ConcreteFunction\n",
    "    frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "    frozen_func.graph.as_graph_def()\n",
    "\n",
    "    layers = [op.name for op in frozen_func.graph.get_operations()]\n",
    "    if verbose:\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Frozen model layers: \")\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Frozen model inputs: \")\n",
    "        print(frozen_func.inputs)\n",
    "        print(\"Frozen model outputs: \")\n",
    "        print(frozen_func.outputs)\n",
    "\n",
    "\n",
    "        \n",
    "    # Save frozen graph from frozen ConcreteFunction to hard drive\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                        logdir=logdir,\n",
    "                        name=name,\n",
    "                        as_text=False)\n",
    "    # as text\n",
    "    name2 = \"frozen_graph_tf2.pbtxt\"\n",
    "    tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                        logdir=logdir,\n",
    "                        name=name2,\n",
    "                        as_text=True)\n",
    "        \n",
    "        \n",
    "def save_model_tf2(model, model_path):\n",
    "    model.save(model_path)\n",
    "\n",
    "    \n",
    "OUT_MODEL_PATH = ''\n",
    "\n",
    "if (TF_VER=='1'):\n",
    "    FROZEN_GRAPH_PATH_TF1 = os.path.join(MODEL_DIR, \"frozen_graph.pb\")\n",
    "    save_model_tf1(model, FROZEN_GRAPH_PATH_TF1)\n",
    "else:\n",
    "    OUT_MODEL_PATH = os.path.join(MODEL_DIR, 'saved_model')\n",
    "    save_model_tf2(model, OUT_MODEL_PATH)\n",
    "    name = \"frozen_graph_tf2.pb\"\n",
    "    FROZEN_GRAPH_PATH_TF2 = os.path.join(MODEL_DIR, name)\n",
    "    saved_frozen_graph_tf2(model, MODEL_DIR, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple case](img/deploy-trained-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the batch size for the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    # set up the figure\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # plot the images: each image is 28x28 pixels\n",
    "    for i in range(50):\n",
    "        ax = fig.add_subplot(5, 10, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(test_images[i,:].reshape((28,28)),cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "        if prediction_values[i] == np.argmax(test_labels[i]):\n",
    "            # label the image with the blue text\n",
    "            ax.text(0, 7, class_names[prediction_values[i]], color='blue')\n",
    "        else:\n",
    "            # label the image with the red text\n",
    "            ax.text(0, 7, class_names[prediction_values[i]], color='red')\n",
    "            \n",
    "            \n",
    "inf = model.predict(x=test_images)\n",
    "prediction_values = np.argmax(inf, axis=-1)\n",
    "visualize()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's measure inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random batch by randomly selecting the index of the first image in the batch\n",
    "image_index = int(np.random.randint(0, test_images.shape[0] - BATCH_SIZE, size=1)[0])\n",
    "random_batch = test_images[image_index:(image_index + BATCH_SIZE)]\n",
    "\n",
    "print(\"Randomly selected index:\", image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the randomly selected batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def visualize_batch(rnd_idx, batch_size, predictions = False):\n",
    "    \n",
    "    subplot_rows = math.ceil(batch_size / 10)\n",
    "    subplot_cols = 10\n",
    "    \n",
    "    # set up the figure\n",
    "    fig = plt.figure(figsize=(15, subplot_rows * 1.5))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # plot the images: each image is 28x28 pixels\n",
    "    j = 0\n",
    "    for i in range(rnd_idx, rnd_idx + batch_size):\n",
    "        ax = fig.add_subplot(subplot_rows, subplot_cols, j + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(test_images[i,:].reshape((28,28)),cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        if predictions is False:\n",
    "            ax.text(0, 7, class_names[np.nonzero(test_labels[i])[0][0]], color='green')\n",
    "        else:\n",
    "            if predictions[j] == class_names[np.nonzero(test_labels[i])[0][0]]:\n",
    "                ax.text(0, 7, predictions[j], color='blue')\n",
    "            else:\n",
    "                ax.text(0, 7, predictions[j], color='red')\n",
    "        j += 1\n",
    "        \n",
    "            \n",
    "        \n",
    "visualize_batch(image_index, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference multiple times to estimate average time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_iter = 50\n",
    "for i in range(warmup_iter):\n",
    "    prediction = model.predict(x=random_batch)\n",
    "\n",
    "loop_count = 200\n",
    "start_time = time.time()\n",
    "for i in range(loop_count):\n",
    "    prediction = model.predict(x=random_batch)\n",
    "    \n",
    "print(\"Unoptimized inference time: %s ms in average\" %((time.time() - start_time)* 1000 / loop_count))\n",
    "\n",
    "predicted_labels = [class_names[np.argmax(prediction, axis=-1)[i]] for i in range(len(prediction))]\n",
    "\n",
    "visualize_batch(image_index, BATCH_SIZE, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the random image index, batch size and the version of TensorFlow used to saved the frozen graph for future comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "print(image_index)\n",
    "print(BATCH_SIZE)\n",
    "print(TF_VER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_NUMBER_PATH = os.path.join(MODEL_DIR, \"rnd_idx.txt\")\n",
    "with open(TMP_NUMBER_PATH, 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. TensorRT optimized inference with ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part does not depend on the TensorFlow version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the onnx converters from the source\n",
    "!pip install --quiet -U git+https://github.com/microsoft/onnxconverter-common\n",
    "!pip install --quiet -U git+https://github.com/onnx/keras-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras2onnx\n",
    "print(\"keras2onnx version is \"+keras2onnx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to ONNX and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = keras2onnx.convert_keras(model, 'fashion-mnist-onnx', debug_mode=1)\n",
    "\n",
    "OUT_ONNX_MODEL = MODEL_DIR + \"/model.onnx\"\n",
    "keras2onnx.save_model(onnx_model, OUT_ONNX_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are not using Keras, but plain TensorFlow, you can use the [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx) for the conversion. If you are training in PyTorch you can use the [torch.onnx](https://pytorch.org/docs/master/onnx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare ONNX runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "sess = onnxruntime.InferenceSession(OUT_ONNX_MODEL, sess_options)\n",
    "data = [random_batch.astype(np.float32)]\n",
    "input_names = sess.get_inputs()\n",
    "feed = dict([(input.name, data[n]) for n, input in enumerate(sess.get_inputs())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's measure inference time. Compare this execution time with unoptimized inference. Execute this cell several times to exclude the GPU 'warming up' effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_iter = 50\n",
    "for i in range(warmup_iter):\n",
    "    onnx_predicted_label = sess.run(None, feed)[0].argmax()\n",
    "\n",
    "loop_count = 200\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(loop_count):\n",
    "    onnx_predicted_label = sess.run(None, feed)[0].argmax(axis=1)\n",
    "print(\"ONNX inference time: %s ms in average\" %((time.time() - start_time)* 1000 / loop_count))\n",
    "\n",
    "predicted_labels = [class_names[onnx_predicted_label[i]] for i in range(len(onnx_predicted_label))]\n",
    "\n",
    "visualize_batch(image_index, BATCH_SIZE, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.get_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU execution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing the default GPU build (version 1.5.3) requires CUDA runtime libraries being installed on the system:\n",
    "Version: CUDA 10.2 and cuDNN 8.0.3. If you are having a newer insrtallation, it won't work.\n",
    "\n",
    "Just for the reference, to use the GPU accelerated onnxruntime, install it as follows \n",
    "\n",
    "```\n",
    "!pip uninstall --quiet onnxruntime -y\n",
    "!pip install --quiet -U onnxruntime-gpu\n",
    "```\n",
    "\n",
    "That will lead to `sess.get_providers()` showing further options, like `'TensorrtExecutionProvider'`. By default the execution context will be switched to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the TensorRT engine from ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you skipped the previous part, install ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if you are starting just from this part, execute the following cells to load the data and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "\n",
    "TRT_LOGGER = tensorrt.Logger(tensorrt.Logger.WARNING)\n",
    "trt_runtime = tensorrt.Runtime(TRT_LOGGER)\n",
    "\n",
    "# ONNX parser only supports networks with an explicit batch dimension\n",
    "explicit_batch = 1 << (int)(tensorrt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def build_engine(onnx_path, shape):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the function to create the TensorRT engine\n",
    "    Args:\n",
    "       onnx_path : Path to onnx_file. \n",
    "       shape : Shape of the input of the ONNX file. \n",
    "    \"\"\"\n",
    "    with tensorrt.Builder(TRT_LOGGER) as builder, builder.create_network(explicit_batch) as network, tensorrt.OnnxParser(network, TRT_LOGGER) as parser:        \n",
    "        builder.max_workspace_size = 8000000000\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            parser.parse(model.read())\n",
    "        network.get_input(0).shape = shape\n",
    "        engine = builder.build_cuda_engine(network)\n",
    "        return engine\n",
    "    \n",
    "    \n",
    "def load_engine(trt_runtime, plan_path):\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_ONNX_MODEL = MODEL_DIR + \"/model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from onnx import ModelProto\n",
    " \n",
    "ONNX_TRT_ENGINE_PATH = MODEL_DIR + \"/trt_onnx_out.plan\"\n",
    "onnx_path = OUT_ONNX_MODEL\n",
    "\n",
    "model = ModelProto()\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "    model.ParseFromString(f.read())\n",
    " \n",
    "    d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
    "    d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
    "    d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
    "    shape = [BATCH_SIZE , d0, d1 ,d2]\n",
    "\n",
    "    with build_engine(onnx_path, shape = shape) as engine:\n",
    "    \n",
    "        # save to file\n",
    "        serialized_engine = engine.serialize()\n",
    "\n",
    "        with open(ONNX_TRT_ENGINE_PATH, \"wb\") as f:\n",
    "            f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference from the TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import numpy as np\n",
    "import pycuda.autoinit \n",
    "\n",
    "\n",
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "# Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) \n",
    "# to hold host inputs/outputs\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = tensorrt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = tensorrt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "\n",
    "# This function is generalized for multiple inputs/outputs for full dimension networks.\n",
    "# (for TensorRT 7.x or higher)\n",
    "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "\n",
    "def load_normalized_test_case(case_num, pagelocked_buffer, batch=1):\n",
    "    _, _, x_test, y_test = load_data()\n",
    "    \n",
    "    if batch > 1:\n",
    "        test_batch = x_test[case_num:(case_num + BATCH_SIZE)].ravel()\n",
    "    else:\n",
    "        test_batch = x_test[case_num].ravel()\n",
    "    np.copyto(pagelocked_buffer, test_batch)\n",
    "    \n",
    "    if batch > 1:\n",
    "        return y_test[case_num:(case_num + BATCH_SIZE)]\n",
    "    else:\n",
    "        return y_test[case_num]\n",
    "\n",
    "\n",
    "def run_trt_inference(context, loop_count = 200, warmup=50):\n",
    "    test_case = np.argmax(load_normalized_test_case(idx, inputs[0].host, BATCH_SIZE), axis=-1)\n",
    "\n",
    "    # run multiple predictions to measure time\n",
    "    for i in range(warmup):\n",
    "        # The do_inference function will return a list of outputs - we only have one in this case.\n",
    "        [pred] = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(loop_count):\n",
    "        # The do_inference function will return a list of outputs - we only have one in this case.   \n",
    "        [pred] = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "\n",
    "    print(\"TRT inference time: %s ms in average\" %((time.time() - start_time) * 1000 / loop_count))  \n",
    "\n",
    "    #print(pred)\n",
    "    pred_resh = np.reshape(pred, (BATCH_SIZE, len(class_names)))\n",
    "    \n",
    "    predicted_labels = [class_names[np.argmax(pred_resh, axis=-1)[i]] for i in range(len(pred_resh))]\n",
    "    visualize_batch(idx, BATCH_SIZE, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the generated engine and execute inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ONNX_TRT_ENGINE_PATH, \"rb\") as f, tensorrt.Runtime(TRT_LOGGER) as trt_runtime:\n",
    "     with trt_runtime.deserialize_cuda_engine(f.read()) as loaded_engine:\n",
    "        # allocate buffers\n",
    "        inputs, outputs, bindings, stream = allocate_buffers(loaded_engine)\n",
    "\n",
    "        # create execution context\n",
    "        with loaded_engine.create_execution_context() as context:\n",
    "            print(\"\\n=== Testing ===\")\n",
    "            run_trt_inference(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. TF-TRT in TensorFlow 2.x (SavedModel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![saved model](img/saved-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You shall be in the TensorFlow 2.x environment to perform this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_VER == '1':\n",
    "    print(\"This part of the tutorial is designed for the TensorFlow 2.x. Please switch the environment.\")\n",
    "\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of the saved model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir $OUT_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to TRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_OUTPUT_PATH = os.path.join(MODEL_DIR, \"optimized_model\")\n",
    "\n",
    "# Here we overwrite the default conversion parameters to suit our needs.\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "    #minimum_segment_size = 3,\n",
    "    #max_batch_size=1,\n",
    "    max_workspace_size_bytes=8000000000\n",
    ")\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(input_saved_model_dir=OUT_MODEL_PATH, conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "converter.save(TRT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trt_test(loaded_model, loop_count = 200, warmup_inter = 50):\n",
    "\n",
    "    for i in range(warmup_inter):\n",
    "        prediction = loaded_model(tf.constant(random_batch))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(loop_count):\n",
    "        prediction = loaded_model(tf.constant(random_batch))\n",
    "\n",
    "    print(\"TF-TRT inferences with %s ms in average\" %((time.time() - start_time) * 1000 / loop_count))\n",
    "    \n",
    "    predicted_labels = [class_names[np.argmax(prediction, axis=-1)[i]] for i in range(len(prediction))]\n",
    "\n",
    "    visualize_batch(idx, BATCH_SIZE, predicted_labels)\n",
    "    \n",
    "    \n",
    "random_batch = test_images[idx:(idx + BATCH_SIZE)]  \n",
    "loaded = tf.saved_model.load(TRT_OUTPUT_PATH)\n",
    "run_trt_test(loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-TRT FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting to TF-TRT FP16...')\n",
    "TRT_OUTPUT_PATH_TFTRT_FP16 = os.path.join(MODEL_DIR, \"TFTRT_FP16\")\n",
    "\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.FP16,\n",
    "    #minimum_segment_size = 3,\n",
    "    #max_batch_size=1,\n",
    "    maximum_cached_engines=8,\n",
    "    max_workspace_size_bytes=8000000000)\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "   input_saved_model_dir=OUT_MODEL_PATH, conversion_params=conversion_params)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir=TRT_OUTPUT_PATH_TFTRT_FP16)\n",
    "\n",
    "print('Done Converting to TF-TRT FP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(TRT_OUTPUT_PATH_TFTRT_FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trt_test(loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-TRT INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input = tf.constant(test_images)\n",
    "print('batched_input shape: ', batched_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Converting to TF-TRT INT8...')\n",
    "TRT_OUTPUT_PATH_TFTRT_INT8 = os.path.join(MODEL_DIR, \"TFTRT_INT8\")\n",
    "\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.INT8, \n",
    "    #minimum_segment_size = 3,\n",
    "    #max_batch_size=1,\n",
    "    max_workspace_size_bytes=8000000000, \n",
    "    use_calibration=True)\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=OUT_MODEL_PATH, \n",
    "    conversion_params=conversion_params)\n",
    "\n",
    "def calibration_input_fn():\n",
    "    yield (batched_input, )\n",
    "converter.convert(calibration_input_fn=calibration_input_fn)\n",
    "\n",
    "converter.save(output_saved_model_dir=TRT_OUTPUT_PATH_TFTRT_INT8)\n",
    "print('Done Converting to TF-TRT INT8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(TRT_OUTPUT_PATH_TFTRT_INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_trt_test(loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with different segment size values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the default parameters of the converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt.DEFAULT_TRT_CONVERSION_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Excercise:</b> in your example, try varying the values of `minimum_segment_size` and `max_batch_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have not noticed any difference in performance after applying quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first reason might be that the model is too small to see any difference (like in the given example). \n",
    "\n",
    "Furthermore, not all hardware can support quantization. Refer to this table to check, if your GPU is capable of this operation: [https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html#hardware-precision-matrix](https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html#hardware-precision-matrix)\n",
    "\n",
    "And finally, not every layer supports different precisions. Check the list of supported precision modes per TensorRT layer here: [https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#layers-precision-matrix](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#layers-precision-matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Converting to UFF in TensorFlow 1.x and exporting the TRT engine (optional part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow 2.x the UFF format has been deprecated, however, it's still being widely used with TensorRT, especially when it comes to custom layer plugins. \n",
    "\n",
    "In order to perform the following part of this tutorial, switch your environment to the one with an older TensorFlow version. Refer to the README.md to learn how to open the jupyter notebook in a different environment.\n",
    "\n",
    "In the newly opened notebook, open the same `trt_python_workflow.ipynb` and continue from this place. You may also keep this document for the reference, but make sure to shutdown its kernel (Kernel -> Shutdown) to free the GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel():\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run kernel_reload.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_VER == '2':\n",
    "    print(\"This part of the tutorial is designed for the TensorFlow 1.x. Please switch the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming to UFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphsurgeon as gs \n",
    "import uff\n",
    "import tensorrt \n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to UFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UFF_MODEL_PATH = './models/out_model.uff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_VER_used_for_train==2:\n",
    "    output_nodes = ['Identity']\n",
    "    dynamic_graph = gs.DynamicGraph(FROZEN_GRAPH_PATH_TF2)\n",
    "else:\n",
    "    output_nodes = ['OutputLayer/Softmax']\n",
    "    dynamic_graph = gs.DynamicGraph(FROZEN_GRAPH_PATH_TF1)\n",
    "\n",
    "uff_model = uff.from_tensorflow(dynamic_graph.as_graph_def(), output_nodes=output_nodes, output_filename=UFF_MODEL_PATH, text=True)\n",
    "print(\"converted to UFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the TRT engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part should be performed on target hardware. \n",
    "\n",
    "It's very important to specify the model data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    INPUT_NAME = \"InputLayer\"\n",
    "    INPUT_SHAPE = (1, 28, 28)\n",
    "    if TF_VER_used_for_train==2:\n",
    "        OUTPUT_NAME = \"Identity\"\n",
    "    else:\n",
    "        OUTPUT_NAME = \"OutputLayer/Softmax\"\n",
    "    DATA_TYPE = tensorrt.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the model file existst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(UFF_MODEL_PATH):\n",
    "    raise IOError(\"\\n{}\\n{}\\n{}\\n\".format(\n",
    "        \"Failed to find the model file ({}).\".format(UFF_MODEL_PATH)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary code to build and use the TRT engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(model_path):\n",
    "    builder = tensorrt.Builder(TRT_LOGGER) \n",
    "    builder.max_batch_size = BATCH_SIZE\n",
    "    network = builder.create_network() \n",
    "\n",
    "    builder_config = builder.create_builder_config()\n",
    "    builder_config.max_workspace_size = 8000000000\n",
    "    #builder_config.set_flag(tensorrt.BuilderFlag.FP16)\n",
    "\n",
    "    parser = tensorrt.UffParser()\n",
    "    parser.register_input(ModelData.INPUT_NAME, ModelData.INPUT_SHAPE)\n",
    "    parser.register_output(ModelData.OUTPUT_NAME)\n",
    "    parser.parse(model_path, network)\n",
    "\n",
    "    return builder.build_engine(network, builder_config)\n",
    "\n",
    "\n",
    "\n",
    "# Simple helper data class that's a little nicer to use than a 2-tuple.\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    \n",
    "# Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) \n",
    "# to hold host inputs/outputs\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = tensorrt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = tensorrt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "\n",
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "\n",
    "\n",
    "def load_normalized_test_case(case_num, pagelocked_buffer, batch=1):\n",
    "    _, _, x_test, y_test = load_data()\n",
    "    \n",
    "    if batch > 1:\n",
    "        test_batch = x_test[case_num:(case_num + BATCH_SIZE)].ravel()\n",
    "    else:\n",
    "        test_batch = x_test[case_num].ravel()\n",
    "    np.copyto(pagelocked_buffer, test_batch)\n",
    "    \n",
    "    if batch > 1:\n",
    "        return y_test[case_num:(case_num + BATCH_SIZE)]\n",
    "    else:\n",
    "        return y_test[case_num]\n",
    "\n",
    "\n",
    "def run_trt_inference(context, loop_count = 200, warmup=50):\n",
    "    \n",
    "    test_case = np.argmax(load_normalized_test_case(idx, inputs[0].host, BATCH_SIZE), axis=-1)\n",
    "\n",
    "    # run multiple predictions to measure time\n",
    "    for i in range(warmup):\n",
    "        # The do_inference function will return a list of outputs - we only have one in this case.\n",
    "        [pred] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream, batch_size=BATCH_SIZE)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(loop_count):\n",
    "        # The do_inference function will return a list of outputs - we only have one in this case.   \n",
    "        [pred] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"TRT inference time: %s ms in average\" %((time.time() - start_time) * 1000 / loop_count)) \n",
    "\n",
    "    #print(pred)\n",
    "    pred_resh = np.reshape(pred, (BATCH_SIZE, len(class_names)))\n",
    "    \n",
    "    predicted_labels = [class_names[np.argmax(pred_resh, axis=-1)[i]] for i in range(len(pred_resh))]\n",
    "    visualize_batch(idx, BATCH_SIZE, predicted_labels)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and use the engine for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = tensorrt.Logger(tensorrt.Logger.VERBOSE)\n",
    "\n",
    "with build_engine(UFF_MODEL_PATH) as engine:\n",
    "    \n",
    "    # save to file\n",
    "    serialized_engine = engine.serialize()\n",
    "\n",
    "    TRT_ENGINE_PATH = \"models/out_model.engine\"\n",
    "\n",
    "    with open(TRT_ENGINE_PATH, \"wb\") as f:\n",
    "        f.write(serialized_engine)\n",
    "    \n",
    "    # allocate buffers\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "    \n",
    "    # create execution context\n",
    "    with engine.create_execution_context() as context:\n",
    "        print(\"\\n=== Testing ===\")\n",
    "        run_trt_inference(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the engine from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRT_ENGINE_PATH, \"rb\") as f, tensorrt.Runtime(TRT_LOGGER) as runtime:\n",
    "     with runtime.deserialize_cuda_engine(f.read()) as loaded_engine:\n",
    "        # allocate buffers\n",
    "        inputs, outputs, bindings, stream = allocate_buffers(loaded_engine)\n",
    "\n",
    "        # create execution context\n",
    "        with loaded_engine.create_execution_context() as context:\n",
    "            print(\"\\n=== Testing ===\")\n",
    "            run_trt_inference(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
